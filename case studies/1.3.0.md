## Current State
With Version 1.2.2 released and in use there is already a large codebase to use as a start. Right now the Outpost project manager has two major sides, the Calendar side, and the Outpost Project Database Side. The calendar side looks at any selected calendar booking and tries to find the Reconciliation Sheet the employees use to log their work on. It simply serves as a quicker link to the sheet, you still need to log all your information manually. On the Outpost Project Database side, it will load whatever project you have selected, and find any information it can on that project, find the folder, the reconciliation sheet, the costing sheet, and the proposal document, and will present a link to anything it can find. It also facilitates making new projects and generating those documents automatically as well. 

#### The Next Update (Version 1.3)
On the agenda for the next update all revolve around making a more efficient system. Everything works as expected right now, but the app is quite slow. I figure by looking at exactly what is the slowest part of the app, and caching the information it is resolving, I can speed up the app as a whole. I think the slowest thing is just the calls to access the spreadsheet, so it should be as easy as saving those values into the cache and as properties. The first step in properly improving the speed of my code, is to properly benchmark everything.

### Next Up: Benchmarking.
So I don't just want to know how fast everything is, but I really want to make a system to measure the speed of everything at once all at once as I make changes to how data flows through the app. For the OPD side, that should be easy, I can add a button that asks the user if they want to run some benchmark tests, and then just have it go through everything I want to test, setting up the right situation first. On the calendar side of things it will be harder because there is nothing about the app that currently has the ability to set up or move calendar entries, and I don't want to add that permission just for this test. Because the app is split up, I don't have to worry about these sides interacting in bad ways so I can just start with the OPD and then worry about the Calendar part after.

### OPD Benchmarking Button
The easiest thing is to just add a button right in the footer, that way it can access all of the functions of the frontend and I can measure the frontend separately from the backend. I just have to make sure no one else gets the button besides me.
#### Step 1: Checking if the user is a developer.
I already made a user class that I used really just to get the name of the user to check something, but now I'll go ahead and make a isAdmin() and isDeveloper() method to the user class. I'll add my email and information as a property in the apps script project to abstract that information out of others sight, and then ill have the functions check properties that are lists of names:
```
  static get isAdmin(): boolean {
    const email = User.email;
    let isAdmin = false; 
    exports.properties.getProperty('administrators').split(',').forEach((adminEmail: string) => {
      if (email == adminEmail) {
        isAdmin = true;
      }
    });
    return isAdmin;
  }

  static get isDeveloper(): boolean {
    const email = User.email;
    let isDeveloper = false;
    exports.properties.getProperty('developers').split(',').forEach((developerEmail: string) => {
      if (email === developerEmail) {
        isDeveloper = true;
      }
    });
    return isDeveloper;
  }
```
#### Step 2: Building the HTML
I want to make it so that rather that just loading up the prebuilt HTML file and serve that, I am going to first check if the user is a developer with our new function, then I'll Build the html and add the developer button if they are. Ill just split it up in the footer, but I'll leave room for doing the same thing with admins, cause I'll want to give admins their own functionality at some point. In version 1.2.2 the footer looked like this:

![OPD Footer](images/1.2.2OPDFooter.png)

After our changes to our sidebar function here: 
```
export function openOPDSidebar(): void {
  const output = HtmlService.createTemplateFromFile('src/html/baseStyle').evaluate();
  output.append(HtmlService.createHtmlOutputFromFile('src/opd/html/sidebar').getContent());
  if (User.isAdmin) output.append(HtmlService.createHtmlOutputFromFile('src/opd/html/adminSidebar').getContent());
  output.append('<div class="sidebar bottom width-100">');
  output.append(HtmlService.createHtmlOutputFromFile('src/opd/html/sidebarBottom').getContent());
  if (User.isDeveloper) output.append(HtmlService.createHtmlOutputFromFile('src/opd/html/devSidebar').getContent());
  output.append(HtmlService.createHtmlOutputFromFile('src/html/footer').getContent());
  output.append('</div>');
  output.append(HtmlService.createHtmlOutputFromFile('src/opd/html/sidebarjs').getContent());
  output.append('</body></html>');
  output
    .setTitle('Outpost Project Manager')
    .setWidth(1000)
    .setSandboxMode(HtmlService.SandboxMode.IFRAME);
  SpreadsheetApp.getUi().showSidebar(output);
}
```
The footer now has this button when I are a dev:

![OPD Footer](images/1.3.0RunBenchmarkButton.png)

#### Step 3: Making the button work
Building the HTML is the easy part, now I need to make sure the button actually pulls what I want. 
##### Step 3a: How should the data be formatted
In the end I'll be storing all of the data in a JSON file, I don't want to deal with normalization of a 2D database and this dataset will be pretty small, so I've defined a structure for the JSON including a list of tests I want to have benchmarked in the interfaces file. It got rather long so I've Broken it out into chunks:
```
export interface BasicTestJSON {
  'Raw': [
    {[key: string]: number}?
  ],
  'Statistics'?: {
    'Mean Total': number,
    'Mean Per Process'?: {[key: string]: number},
    'Range Total': number,
    'Range Per Process'?: {[key: string]: number}
  }
}

export interface OPDSheetJSONTests {
  'jumpToProjects'?: BasicTestJSON,
  'jumpToProposals'?: BasicTestJSON,
  'getInitiative from empty project'?: BasicTestJSON,
  'getInitiative from empty proposal'?: BasicTestJSON,
  'getInitiative from proposal with all docs'?: BasicTestJSON,
  'getInitiative from project with all docs'?: BasicTestJSON,
  'getInitiative from proposal with no docs'?: BasicTestJSON,
  'getInitiative from project with no docs'?: BasicTestJSON,
  'generateProposal from existing client'?: BasicTestJSON,
  'generateProposal from new client'?: BasicTestJSON,
  'acceptProposal'?: BasicTestJSON,
  'generateProject from existing client'?: BasicTestJSON,
  'generateProject from new client'?: BasicTestJSON,
  'openChangelog'?:  BasicTestJSON
}

export interface CalendarJSONTests {
  'openChangelog'?: BasicTestJSON,
  'getEvent from new event'?: BasicTestJSON,
  'getEvent from never loaded event with reconciliation sheet'?: BasicTestJSON,
  'getEvent from never loaded event without reconciliation sheet'?: BasicTestJSON,
  'getEvent from loaded event with reconciliation sheet'?: BasicTestJSON,
  'getEvent from loaded event without reconciliation sheet'?: BasicTestJSON
}

export interface BenchmarkJSON {
  'OPDSheet'?: {
    'Frontend': OPDSheetJSONTests,
    'Backend': OPDSheetJSONTests
  },
  'Calendar'?: {
    'Frontend': CalendarJSONTests,
    'Backend': CalendarJSONTests
  }
}
```
##### Step 3b: Linking the function to the button.
So now I basically have a running list of the thing the button needs to do in the OPDSheetJSONTests interface. Google says its best practice to separate your JavaScript from your html files on the frontend, which I did for the main file, but for this devSidebar addition its only one button with a lot of JavaScript attached, so I've put it all in one file. The button is pretty simple at the top of the file:
```
<button id="benchmark-button" class="create full-width row">
  <span class="material-symbols-outlined">
    speed
  </span>
  Run Benchmark
  <span class="material-symbols-outlined">
    speed
  </span>
</button>
```
Now I can grab that with jQuery and make the button run the requestBenchmark function on click. I don't want the benchmark to just run automatically because its a long slow intrusive process, so I'll Have it first verify I meant to click the button.
```
$(function() {
  $('#benchmark-button').click(requestBenchmark);
});
```
##### Step 3c: Establishing the loop.
I want it to go through the entire list of functions, but I want it to do it multiple times, I'm not gonna get any real statistically relevant number of runs, but ill want at least 3 or 5 runs of each test to average away any noise. So the first thing it should do it ask how many times the function should run, then it should begin the loop of the tests function. The tests function should return a full OPDSheetJSONTests Object. that should be put into a merge function where it will be merged with a full BenchmarkJSON Object.
```
async function requestBenchmark() {
  let iterationCount = askForIterationCount();
  let result = confirm('Are you sure you want to run the benchmark ' + iterationCount + ' times?');
  if (result == false) {
    console.warn('Benchmark Haulted');
    return;
  }
  let fullBenchmark = {
    'OPDSheet': {
      'Frontend': {}
    }
  }
  while(iterationCount > 0) {
    let results = await runBenchmarkTests();
    mergeBenchmarkResults(fullBenchmark, results);
    iterationCount--;
  }
  toastr.success('Benchmark complete:', fullBenchmark);
  console.debug('Benchmark complete:', fullBenchmark);
}

function askForIterationCount() {
  var iterationCount = prompt('How many iterations would you like to run?');
  if (typeof iterationCount !== 'number') {
    iterationCount = parseInt(iterationCount);
    if (iterationCount == NaN) {
      alert('Invalid input. Please enter a number.');
      return askForIterationCount();
    }
  }
  if (iterationCount <= 0) {
    alert('Invalid input. Please enter a positive number.');
    return askForIterationCount();
  }
  return iterationCount;
}

function mergeBenchmarkResults(fullBenchmark, results) {
  for (const key of Object.keys(results)) {
    if (fullBenchmark['OPDSheet']['Frontend'][key]) {
      fullBenchmark['OPDSheet']['Frontend'][key]['Raw'].push(results[key]);
    } else {
      fullBenchmark['OPDSheet']['Frontend'][ket] = {
        'Raw': [results[key]]
      }
    }
  }
  return fullBenchmark; 
}
```

##### Step 3d: The runBenchmarkTests() function
So basically the essence of what's happening here is its going to 
1. Ask the server to do any setup needed to run the test
2. Wait a moment for everything to initialize. I do have all api calls put into async functions that I will await, but I found waiting an extra moment after is good to let everything fully update.
3. Set the function calls to benchmark the next function. This sets a trigger to tell the backend to record the benchmark as well, that way at the end I can combine it all and show both the frontend and the backend results.
4. Log the time when I start
5. Run the function I am testing
6. Log the time I finish
7. Record the time delta

That's the entire idea, but there are a lot of individual tests with custom actions for setup so its easier said than done. When all was said and done this was the code I had, knowing I could have abstracted much of each test into a higher order function for code organization, I may do that in the future.
```
async function benchmarkFunction(func) {
  benchmarkNextFunction = true;
  let start = performance.now();
  await func();
  let end = performance.now();
  return end - start;
}

function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

async function runBenchmarkTests() {
  try {
    let results = {};
    results['jumpToProjects'] = await benchmarkFunction(jumpToProject);
    console.debug('jumpToProjects', results['jumpToProjects']);
    toastr.success('jumpToProjects', results['jumpToProjects']);

    startLoading('Selecting Empty Project');
    await selectEmptyProject();
    await sleep(5000);
    stopLoading();
    results['getInitiative from empty project'] = await benchmarkFunction(refreshSidebar);
    console.debug('getInitiative from empty project', results['getInitiative from empty project']);
    toastr.success('getInitiative from empty project', results['getInitiative from empty project']);

    startLoading('Filling in Blank Information');
    await selectNoDocsProject();
    await sleep(5000);
    stopLoading();
    results['getInitiative from project with no docs'] = await benchmarkFunction(refreshSidebar);
    console.debug('getInitiative from project with no docs', results['getInitiative from project with no docs']);
    toastr.success('getInitiative from project with no docs', results['getInitiative from project with no docs']);

    startLoading('Prepare to Generate Project');
    await sleep(1000);
    stopLoading();
    results['generateProject from existing client'] = await benchmarkFunction(generateJob);
    console.debug('generateProject from existing client', results['generateProject from existing client']);
    toastr.success('generateProject from existing client', results['generateProject from existing client']);

    startLoading('Preparing to Reload Project');
    await sleep(1000);
    stopLoading();
    results['getInitiative from project with all docs'] = await benchmarkFunction(refreshSidebar);
    console.debug('getInitiative from project with all docs', results['getInitiative from project with all docs']);
    toastr.success('getInitiative from project with all docs', results['getInitiative from project with all docs']);
    
    startLoading('Deleting Project and Client Files');
    await deleteProjectFiles();
    await deleteClientFiles();
    await selectNoDocsProject();
    await sleep(5000);
    stopLoading();
    results['generateProject from new client'] = await benchmarkFunction(generateJob);
    console.debug('generateProject from new client', results['generateProject from new client']);
    toastr.success('generateProject from new client', results['generateProject from new client']);

    startLoading('Deleting Project Files');
    await deleteProjectFiles();
    // delete the text here too in another function, or maybe just the one function
    stopLoading();
    results['jumpToProposals'] = await benchmarkFunction(jumpToProposal);
    console.debug('jumpToProposals', results['jumpToProposals']);
    toastr.success('jumpToProposals', results['jumpToProposals']);

    startLoading('Selecting Empty Proposal');
    await selectEmptyProposal();
    await sleep(5000);
    stopLoading();
    results['getInitiative from empty proposal'] = await benchmarkFunction(refreshSidebar);
    console.debug('getInitiative from empty proposal', results['getInitiative from empty proposal']);
    toastr.success('getInitiative from empty proposal', results['getInitiative from empty proposal']);

    startLoading('Filling in Blank Information');
    await selectNoDocsProposal();
    await sleep(5000);
    stopLoading();
    results['getInitiative from proposal with no docs'] = await benchmarkFunction(refreshSidebar);
    console.debug('getInitiative from proposal with no docs', results['getInitiative from proposal with no docs']);
    toastr.success('getInitiative from proposal with no docs', results['getInitiative from proposal with no docs']);

    startLoading('Prepare to Generate Proposal');
    await sleep(1000);
    stopLoading();
    results['generateProposal from existing client'] = await benchmarkFunction(generateProposal);
    console.debug('generateProposal from existing client', results['generateProposal from existing client']);
    toastr.success('generateProposal from existing client', results['generateProposal from existing client']);

    startLoading('Preparing to Reload Proposal');
    await sleep(1000);
    stopLoading();
    results['getInitiative from proposal with all docs'] = await benchmarkFunction(refreshSidebar);
    console.debug('getInitiative from proposal with all docs', results['getInitiative from proposal with all docs']);
    toastr.success('getInitiative from proposal with all docs', results['getInitiative from proposal with all docs']);

    startLoading('Deleting Proposal and Client Files');
    await deleteProposalFiles();
    await deleteClientFiles();
    await selectNoDocsProposal();
    await sleep(5000);
    stopLoading();
    results['generateProposal from new client'] = await benchmarkFunction(generateProposal);
    console.debug('generateProposal from new client', results['generateProposal from new client']);
    toastr.success('generateProposal from new client', results['generateProposal from new client']);

    startLoading('Prepare to Accept Proposal');
    await sleep(1000);
    stopLoading();
    results['acceptProposal'] = await benchmarkFunction(acceptProposal);
    console.debug('acceptProposal', results['acceptProposal']);
    toastr.success('acceptProposal', results['acceptProposal']);

    startLoading('Deleting Project Files');
    await deleteProjectFiles();
    stopLoading();

    results['openChangelog'] = await benchmarkFunction(openChangelog);
    console.debug('openChangelog', results['openChangelog']);
    toastr.success('openChangelog', results['openChangelog']);

    return results;
  }  catch (error) {
    stopLoading();
    console.error('Error selecting empty project: ' + error.message);
    alert('Error selecting empty project: ' + error.message);
    return;
  }
}
```
Below were a series of definitions that just called the server to run those functions, and as well most of the tests are functions that already existed here in the frontend, not in the devSidebar.html file, but in the sidebarjs.html file, which will end up both on the frontend side by side at runtime anyway. With all of that in place we should go ahead and add a console.log statement to print the results of the runBenchmarkTests() function and make sure it looks like how we are expecting.

#### Test 1: OPD Benchmarks
So we have that much in place, lets make sure it prints how we want. After a few crashes and fixes to functions it completed without an error.
```
Benchmark results: 
Object
  acceptProposal: 22256.5
  generateProject from existing client: 39690.09999996424
  generateProject from new client: 16429.80000001192
  generateProposal from existing client: 17739.900000035763
  generateProposal from new client: 19731.799999952316
  getInitiative from empty project: 1878.0999999642372
  getInitiative from empty proposal: 2642.699999988079
  getInitiative from project with all docs: 3427.199999988079
  getInitiative from project with no docs: 3722.199999988079
  getInitiative from proposal with all docs: 2974.300000011921
  getInitiative from proposal with no docs: 3457.099999964237
  jumpToProjects: 6858.900000035763
  jumpToProposals: 4984.599999964237
  openChangelog: 1095.699999988079
```
Sure is pretty slow huh, but at least it works. That last thing it said 'Benchmark results' was the last code that was run. If we look for that in the code that means we stopped somewhere in here:
```
while(iterationCount > 0) {
  let results = await runBenchmarkTests();
  console.debug('Benchmark results:', results);
  mergeBenchmarkResults(fullBenchmark, results);
  iterationCount--;
}
```
Turns out it was just a typo in the mergeBenchmarkResults function. So when we fix that we get:
```
Benchmark complete: 
{OPDSheet: {â€¦}}
OPDSheet: 
  Frontend: 
    acceptProposal: 
      Raw: [19338.600000023842]
    generateProject from existing client: 
      Raw: [13848]
    generateProject from new client: 
      Raw: [17805.700000047684]
    generateProposal from existing client: 
      Raw: [19077.80000001192]
    generateProposal from new client: 
      Raw: [20853.399999976158]
    getInitiative from empty project: 
      Raw: [1727.3999999761581]
    getInitiative from empty proposal: 
      Raw: [1576.4000000357628]
    getInitiative from project with all docs: 
      Raw: [4182.199999988079]
    getInitiative from project with no docs: 
      Raw: [3617.399999976158]
    getInitiative from proposal with all docs: 
      Raw: [3038.899999976158]
    getInitiative from proposal with no docs: 
      Raw: [5188.300000011921]
    jumpToProjects: 
      Raw: [6954.800000011921]
    jumpToProposals: 
      Raw: [5953.200000047684]
    openChangelog: 
      Raw: [1244.7999999523163]
```
Great, everything is looking how we expected. I'm going to try and run it with more than one cycle, lets try and get results from 5 runs.